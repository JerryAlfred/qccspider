{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries \n",
    "\n",
    "import requests\n",
    "import lxml\n",
    "import sys\n",
    "\n",
    "# 爬虫\n",
    "from bs4 import BeautifulSoup\n",
    "import xlwt\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function, 之后要用的，把中文字弄成一排 list \n",
    "\n",
    "def get_chinese_list(lst):\n",
    "    temp = []\n",
    "    for item in lst:\n",
    "        temp.append(item.text+\"; \")\n",
    "    return (list(set(temp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在搜索，请稍后\n",
      "NO. 1 key_word 七台河利丰源商贸有限公司\n",
      "re https://www.qcc.com/search?key=%E4%B8%83%E5%8F%B0%E6%B2%B3%E5%88%A9%E4%B8%B0%E6%BA%90%E5%95%86%E8%B4%B8%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8\n",
      "请求都不让，这企查查是想逆天吗？？？\n",
      "好像被拒绝访问了呢...请稍后再试叭...\n",
      "NO. 2 key_word 三亚凤凰惠隆冷饮商行\n",
      "re https://www.qcc.com/search?key=%E4%B8%89%E4%BA%9A%E5%87%A4%E5%87%B0%E6%83%A0%E9%9A%86%E5%86%B7%E9%A5%AE%E5%95%86%E8%A1%8C\n",
      "请求都不让，这企查查是想逆天吗？？？\n",
      "好像被拒绝访问了呢...请稍后再试叭...\n",
      "NO. 3 key_word 三明合林贸易有限公司\n",
      "re https://www.qcc.com/search?key=%E4%B8%89%E6%98%8E%E5%90%88%E6%9E%97%E8%B4%B8%E6%98%93%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f6b5a3c1fea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# craw function 调用之前写好的程序\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f6b5a3c1fea9>\u001b[0m in \u001b[0;36mcraw\u001b[0;34m(url, key_word, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mis_new_proxy_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[1;32m    804\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             ssl_context=context)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_fingerprint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Python 2.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mload_verify_locations\u001b[0;34m(self, cafile, capath, cadata)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mcapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mload_verify_locations\u001b[0;34m(self, cafile, capath)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         load_result = _lib.SSL_CTX_load_verify_locations(\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "\n",
    "\n",
    "# 一定要换上你自己的 cookie \n",
    "\n",
    "def craw(url,key_word,x):\n",
    "    global count \n",
    "    \n",
    "    User_Agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:56.0) Gecko/20100101 Firefox/56.0'\n",
    "    re = r'https://www.qcc.com/search?key='+(urllib.parse.quote(key_word))\n",
    "    print (\"NO.\", count, \"key_word\", key_word)\n",
    "    count+=1 \n",
    "    print (\"re\", re)\n",
    "    headers = {\n",
    "            'Host':'www.qcc.com',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Accept':r'text/html, */*; q=0.01',\n",
    "            'X-Requested-With': 'XMLHttpRequest',\n",
    "            'User-Agent':r'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "            'Referer': re,\n",
    "            'Accept-Encoding':'gzip, deflate, br',\n",
    "            'Accept-Language':'zh-CN,zh;q=0.9',\n",
    "            'Cookie': r'！！！换成你自己的 cookie！！！',\n",
    "            }\n",
    " \n",
    "    try:\n",
    "        response = requests.get(url,headers = headers)\n",
    "        if response.status_code != 200:\n",
    "            response.encoding = 'utf-8'\n",
    "            print(response.status_code)\n",
    "            print('ERROR')    \n",
    "        soup = BeautifulSoup(response.text,'lxml')\n",
    "    except Exception:\n",
    "        print('请求都不让，这企查查是想逆天吗？？？')\n",
    "    try:\n",
    "        com_all_info = soup.find_all(class_='m_srchList')[0].tbody\n",
    "        com_all_info_array = com_all_info.select('tr')\n",
    "        \n",
    "        print('开始爬取数据，请勿打开excel')\n",
    "        \n",
    "        for i in range(0,1):\n",
    "                temp_g_name = com_all_info_array[i].select('td')[2].select('.ma_h1')[0].text    #获取公司名\n",
    "                temp_g_tag = com_all_info_array[i].select('td')[2].select('.search-tags')[0].text    #获取公司标签\n",
    "                temp_r_name = com_all_info_array[i].select('td')[2].select('p')[0].a.text    #获取法人名\n",
    "                temp_g_money = com_all_info_array[i].select('td')[2].select('p')[0].select('span')[0].text.strip('注册资本：')    #获取注册资本\n",
    "                temp_g_date = com_all_info_array[i].select('td')[2].select('p')[0].select('span')[1].text.strip('成立日期：')    #获取公司注册时间\n",
    "                temp_r_email = com_all_info_array[i].select('td')[2].select('p')[1].text.split('\\n')[1].strip().strip('邮箱：')    #获取法人Email\n",
    "                temp_r_phone = com_all_info_array[i].select('td')[2].select('p')[1].select('.m-l')[0].text.strip('电话：')    #获取法人手机号\n",
    "                temp_g_addr = com_all_info_array[i].select('td')[2].select('p')[2].text.strip().strip('地址：')    #获取公司地址\n",
    "                temp_g_state = com_all_info_array[i].select('td')[3].select('.nstatus.text-success-lt.m-l-xs')[0].text.strip()  #获取公司状态\n",
    "                \n",
    "                website = com_all_info_array[i].find_all(\"a\", class_='ma_h1')[0].get(\"href\")\n",
    "                website = \"https://www.qcc.com/\" + website\n",
    "                \n",
    "                print(\"$$$$$$$$$$$\", website)\n",
    "                                \n",
    "                response = requests.get(website,headers = headers)\n",
    "                if response.status_code != 200:\n",
    "                    response.encoding = 'utf-8'\n",
    "                    print(response.status_code)\n",
    "                    print('ERROR')    \n",
    "                soup = BeautifulSoup(response.text,'lxml')\n",
    "        \n",
    "                temp_g_gudong = get_chinese_list(soup.find_all(\"h3\", class_='seo font-14'))\n",
    "                \n",
    "                print (\"GUDONG: \", temp_g_gudong)\n",
    "                \n",
    "                g_name_list.append(temp_g_name)\n",
    "                g_tag_list.append(temp_g_tag)\n",
    "                r_name_list.append(temp_r_name)\n",
    "                g_money_list.append(temp_g_money)\n",
    "                g_date_list.append(temp_g_date)\n",
    "                r_email_list.append(temp_r_email)\n",
    "                r_phone_list.append(temp_r_phone)\n",
    "                g_addr_list.append(temp_g_addr)\n",
    "                g_state_list.append(temp_g_state)\n",
    "                g_website_list.append(website) \n",
    "                g_gudong_list.append(temp_g_gudong)\n",
    "                g_num_qcc.append(len(com_all_info_array))\n",
    "\n",
    "    except Exception:\n",
    "        no_result_list.append(key_word)\n",
    "        print('好像被拒绝访问了呢...请稍后再试叭...')\n",
    "         \n",
    "if __name__ == '__main__':\n",
    "    global g_name_list\n",
    "    global g_tag_list\n",
    "    global r_name_list\n",
    "    global g_money_list\n",
    "    global g_date_list\n",
    "    global r_email_list\n",
    "    global r_phone_list\n",
    "    global g_addr_list\n",
    "    global g_state_list\n",
    "    global g_gudong_list\n",
    "    global g_website_list\n",
    "    global g_original_name\n",
    "    global g_num_qcc\n",
    "    global no_result_list\n",
    "    \n",
    "    g_name_list=[]\n",
    "    g_tag_list=[]\n",
    "    r_name_list=[]\n",
    "    g_money_list=[]\n",
    "    g_date_list=[]\n",
    "    r_email_list=[]\n",
    "    r_phone_list=[]\n",
    "    g_addr_list=[]\n",
    "    g_state_list=[]\n",
    "    g_gudong_list = []\n",
    "    g_website_list = []\n",
    "    g_original_name = []\n",
    "    g_num_qcc = []\n",
    "    no_result_list = []\n",
    "\n",
    "    \n",
    "    num = 2 # 想检索的次数 \n",
    "    sleep_time = 2.1 # 每次检索延时的秒数，不要太快，不然有可能被封号\n",
    "     \n",
    "    print('正在搜索，请稍后')\n",
    "    \n",
    "    # 我的所有数据是从一个叫 data 的文件夹里面拿出来的\n",
    "    df = pd.read_excel ('data/经销商+交易对手方删了最初的.xlsx')\n",
    "    \n",
    "    # ’df.Client‘ 是我之前找好的，是我想要查的东西：\n",
    "    company_list = df.Client\n",
    "        \n",
    "    for i in company_list:\n",
    "\n",
    "        for x in range(1,num):\n",
    "                            \n",
    "            url = r'https://www.qcc.com/search_index?key={}&ajaxflag=1&p={}&'.format(urllib.parse.quote(i),x)\n",
    "            \n",
    "            # craw function 调用之前写好的程序\n",
    "            s1 = craw(url,i,x)\n",
    "                        \n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        \n",
    "    workbook = xlwt.Workbook()\n",
    "    #创建sheet对象，新建sheet\n",
    "    sheet1 = workbook.add_sheet('企查查数据', cell_overwrite_ok=True)\n",
    "    #---设置excel样式---\n",
    "    #初始化样式\n",
    "    style = xlwt.XFStyle()\n",
    "    #创建字体样式\n",
    "    font = xlwt.Font()\n",
    "    font.name = '仿宋'\n",
    "#    font.bold = True #加粗\n",
    "    #设置字体\n",
    "    style.font = font\n",
    "    #使用样式写入数据\n",
    "    print('正在存储数据，请勿打开excel')\n",
    "    #向sheet中写入数据\n",
    "    name_list = ['公司名字', '原本表格里面的文字', '企查查搜索结果数', '公司标签','法定法人','注册资本','成立日期','法人邮箱','法人电话','公司地址','公司状态','股东', '网站']\n",
    "\n",
    "    \n",
    "    for cc in range(0,len(name_list)):\n",
    "        sheet1.write(0,cc,name_list[cc],style)\n",
    "    for i in range(0,len(g_name_list)):\n",
    "        print(g_name_list[i])\n",
    "        sheet1.write(i+1,0,g_name_list[i],style)#公司名字\n",
    "        sheet1.write(i+1,1,g_original_name[i],style)#原本表格里面的文字\n",
    "        sheet1.write(i+1,2,g_num_qcc[i],style)#企查查搜索结果数\n",
    "        sheet1.write(i+1,3,g_tag_list[i],style)#公司标签\n",
    "        sheet1.write(i+1,4,r_name_list[i],style)#法定法人\n",
    "        sheet1.write(i+1,5,g_money_list[i],style)#注册资本\n",
    "        sheet1.write(i+1,6,g_date_list[i],style)#成立日期\n",
    "        sheet1.write(i+1,7,r_email_list[i],style)#法人邮箱\n",
    "        sheet1.write(i+1,8,r_phone_list[i],style)#法人电话\n",
    "        sheet1.write(i+1,9,g_addr_list[i],style)#公司地址\n",
    "        sheet1.write(i+1,10,g_state_list[i],style)#公司状态\n",
    "        sheet1.write(i+1,11,g_gudong_list[i],style)#股东信息        \n",
    "        sheet1.write(i+1,12,g_website_list[i],style)#网站\n",
    "\n",
    "        \n",
    "    #保存excel文件，有同名的直接覆盖\n",
    "    workbook.save(r\"desktop\"+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()) +\".xls\")\n",
    "    # workbook.save(r\"D:\\wyy-qcc-\"+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()) +\".xls\")\n",
    "    print('保存完毕~')\n",
    "    print(\"没有搜索结果的是\", no_result_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是 beautiful scrape 的实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以一个静态 html 作为例子\n",
    "\n",
    "html = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"><!-- Elsie --></a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">There will have more story</p>\n",
    "\"\"\"\n",
    "\n",
    "# soup = BeautifulSoup(open('天水金河源商贸有限公司-赵卫军_企业工商信息查询-企查查.htm'))\n",
    "soup = BeautifulSoup(open('阿里巴巴（中国）网络技术有限公司-戴珊_企业工商信息查询-企查查.htm'))\n",
    "\n",
    "print (soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3 class=\"seo font-14\">淘宝（中国）软件有限公司</h3>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 具体看怎么去爬\n",
    "\n",
    "soup.find_all(\"h3\", class_='seo font-14')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'淘宝（中国）软件有限公司'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"h3\", class_='seo font-14')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup.title:  <title>The Dormouse's story</title>\n",
      "soup.title.name:  title\n",
      "soup.title.string:  The Dormouse's story\n",
      "soup.title.parent.name:  head\n",
      "soup.p:  <p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n",
      "soup.p['class']:  ['title']\n",
      "soup.a['href']:  http://example.com/elsie\n",
      "[<b>The Dormouse's story</b>]\n",
      "<a href=\"http://www.baidu.com/\" id=\"link1\" name=\"百度\"><!-- Elsie --></a>\n",
      "[<a href=\"http://www.baidu.com/\" id=\"link1\" name=\"百度\"><!-- Elsie --></a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      ",\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "There will have more story\n",
      "\n",
      "{'href': 'http://www.baidu.com/', 'id': 'link1', 'name': '百度'}\n",
      "http://www.baidu.com/\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n",
      "<b>The Dormouse's story</b>\n",
      "body\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# 爬虫的1个例子\n",
    "\n",
    "#输出第一个 title 标签\n",
    "print (\"soup.title: \", soup.title)\n",
    " \n",
    "#输出第一个 title 标签的标签名称\n",
    "print (\"soup.title.name: \", soup.title.name)\n",
    " \n",
    "#输出第一个 title 标签的包含内容\n",
    "print (\"soup.title.string: \", soup.title.string)\n",
    " \n",
    "#输出第一个 title 标签的父标签的标签名称\n",
    "print (\"soup.title.parent.name: \", soup.title.parent.name)\n",
    " \n",
    "#输出第一个  p 标签\n",
    "print (\"soup.p: \", soup.p)\n",
    " \n",
    "#输出第一个  p 标签的 class 属性内容\n",
    "print (\"soup.p['class']: \", soup.p['class'])\n",
    " \n",
    "#输出第一个  a 标签的  href 属性内容\n",
    "print (\"soup.a['href']: \", soup.a['href'])\n",
    "\n",
    "'''\n",
    "soup的属性可以被添加,删除或修改. 再说一次, soup的属性操作方法与字典一样\n",
    "'''\n",
    "\n",
    "#修改第一个 a 标签的href属性为 http://www.baidu.com/\n",
    "soup.a['href'] = 'http://www.baidu.com/'\n",
    " \n",
    "#给第一个 a 标签添加 name 属性\n",
    "soup.a['name'] = u'百度'\n",
    " \n",
    "#删除第一个 a 标签的 class 属性为\n",
    "del soup.a['class']\n",
    " \n",
    "##输出第一个  p 标签的所有子节点\n",
    "print (soup.p.contents)\n",
    "       \n",
    "#输出第一个  a 标签\n",
    "print (soup.a)\n",
    " \n",
    "#输出所有的  a 标签，以列表形式显示\n",
    "print (soup.find_all('a'))\n",
    " \n",
    "#输出第一个 id 属性等于  link3 的  a 标签\n",
    "print (soup.find(id=\"link3\"))\n",
    " \n",
    "#获取所有文字内容\n",
    "print(soup.get_text())\n",
    " \n",
    "#输出第一个  a 标签的所有属性信息\n",
    "print (soup.a.attrs)\n",
    " \n",
    "for link in soup.find_all('a'):\n",
    "    #获取 link 的  href 属性内容\n",
    "    print(link.get('href'))\n",
    " \n",
    "#对soup.p的子节点进行循环输出    \n",
    "for child in soup.p.children:\n",
    "    print(child)\n",
    " \n",
    "#正则匹配，名字中带有b的标签\n",
    "for tag in soup.find_all(re.compile(\"b\")):\n",
    "    print(tag.name)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
